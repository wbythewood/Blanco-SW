{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data for events\n",
    "This part of the code will be used to download event data.\n",
    "\n",
    "It is based on Josh Russell's **fetch_EVENTS**.\n",
    "\n",
    "*william b hawley april 2020*\n",
    "\n",
    "An addition was made to try to even out azimuthal distribution of events. That is done in the following way:\n",
    "1. the user supplies *two* magnitude cutoffs in the config file. The minimum magnitude is used to generate a preliminary list of events that occurred during the desired time window. This will be much larger than the number of events we will use. \n",
    "2. The user additionally supplies the number of backazimuthal bins. The 360 degrees of backazimuth are divided into this number of bins. \n",
    "3. For each event in this initial catalogue, both the magnitude and the backazimuth to the array are stored. \n",
    "4. The events are sorted into backazimuthal bins. \n",
    "5. The number of earthquakes larger than or equal to the second supplied magnitude, LargeEqCutoff, is tallied in each bin. The largest number of these earthquakes in a single bin is the target number of earthquakes we want in every bin. \n",
    "6. For every other bin, we search through the preliminary event list for additional events. We start at the largest magnitudes, and collect events that are in this bin that are progressively smaller, until the total number of events in this bin is equal to the target number. In this way, we create a list of earthquakes we want to use. \n",
    "7. The download step is modified to skip events that are not in this pared down list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from config import *\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import obspy\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "from obspy.geodetics import gps2dist_azimuth, locations2degrees\n",
    "from obspy.io.sac import SACTrace\n",
    "\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "\n",
    "import pyproj\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if data directory exists\n",
    "if not os.path.exists(DataDir):\n",
    "    os.makedirs(DataDir)\n",
    "\n",
    "# load the client\n",
    "client = Client(webservice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 events in catalogue.\n"
     ]
    }
   ],
   "source": [
    "# Load event catalogue\n",
    "t1 = UTCDateTime(tstart)\n",
    "t2 = UTCDateTime(tend)\n",
    "catIRIS = client.get_events(starttime=t1, endtime=t2, minmagnitude=minMag)\n",
    "\n",
    "print((str(len(catIRIS))+\" events in catalogue.\"))\n",
    "\n",
    "# Load stations\n",
    "inventory = client.get_stations(network=network, station=','.join(StaList), channel=','.join(ChanList), starttime=t1, endtime=t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New section to get even azimuthal coverage\n",
    "\n",
    "# we will store event info here\n",
    "EvtList = []\n",
    "EvtListUse = []\n",
    "\n",
    "# and number of earthquakes per bin greater than some value here\n",
    "NumLargeEqs = [0] * NAziBin\n",
    "\n",
    "# loop through catalogue to get event info\n",
    "for iev in range(0,len(catIRIS)):\n",
    "    if isCMT_params: \n",
    "        if isCentroid:\n",
    "            ior = 1\n",
    "        else:\n",
    "            ior = 0\n",
    "        \n",
    "        # if searching CMT catalogue, pull out necessary time info\n",
    "        time = catIRIS[iev].origins[0].time\n",
    "        date = datetime.strptime(str(time),'%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        month = calendar.month_abbr[date.month].lower()\n",
    "        year = str(date.year)\n",
    "        \n",
    "        # in case time is not exactly the same...\n",
    "        time1 = str(time-50) #+/- 50 seconds\n",
    "        time2 = str(time+50)\n",
    "        catCMT = obspy.read_events(\"https://www.ldeo.columbia.edu/~gcmt/projects/CMT/catalog/NEW_MONTHLY/\"+year+\"/\"+month+\"\"+year[2:4]+\".ndk\")\n",
    "        cat_filt = catCMT.filter('time > '+time1, 'time < '+time2, 'magnitude >= '+str(minMag-1))\n",
    "        if len(cat_filt)==0:\n",
    "            #print('Cannot find event in GCMT catalogue... using IRIS')\n",
    "            cat = catIRIS[iev].copy()\n",
    "            ior = 0\n",
    "        else:\n",
    "            cat = cat_filt[0].copy()\n",
    "    else:\n",
    "        cat = catIRIS[iev].copy()\n",
    "        ior = 0\n",
    "    \n",
    "    # earthquake parameters\n",
    "    evT1 = cat.origins[ior].time\n",
    "    evT2 = evT1 + trLen\n",
    "    evdp = cat.origins[ior].depth\n",
    "    evla = cat.origins[ior].latitude\n",
    "    evlo = cat.origins[ior].longitude\n",
    "    mag = cat.magnitudes[0].mag\n",
    "    \n",
    "    # date for naming folders\n",
    "    date = datetime.strptime(str(evT1),'%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    evName = date.strftime('%Y%m%d%H%M')\n",
    "    \n",
    "    # calculate backazimuth\n",
    "    geodesic = pyproj.Geod(ellps='WGS84')\n",
    "    azi,bazi,dist = geodesic.inv(evlo, evla, ArrayLoc[1], ArrayLoc[0])\n",
    "\n",
    "    # put in back azimuth bin\n",
    "    baziNonNeg = bazi\n",
    "    # to make the bins clockwise from north\n",
    "    if baziNonNeg < 0:\n",
    "        baziNonNeg = 360 + bazi\n",
    "    bazBin = (baziNonNeg/360)*NAziBin\n",
    "    bazBin = math.floor(bazBin)\n",
    "\n",
    "    # save all in this structure\n",
    "    EvInfo = [evName,evT1,bazi,bazBin,mag,catIRIS[iev].resource_id] #make sure resource_id remains last here\n",
    "    EvtList.append(EvInfo)\n",
    "    # and if eq is larger than 6.5, add one to the correct bin\n",
    "    if mag >= LargeEqCutoff:\n",
    "        NumLargeEqs[bazBin] += 1\n",
    "        EvtListUse.append([evName,evT1,catIRIS[iev].resource_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using event 201209260017, M5.28 for bin 3\n",
      "done with bin 3\n",
      "Using event 201209260141, M5.1 for bin 4\n",
      "done with bin 4\n",
      "Using event 201209260645, M5.05 for bin 5\n",
      "done with bin 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# here is the function that will be done for each bin\n",
    "def FindMoreEvents(ibin,Neq,EvtList,EvtListUseFunc,LargeEqCutoff):\n",
    "    #EvtListUseFunc = EvtListUse.copy()\n",
    "    # start with no earthquakes found for this bin... \n",
    "    Nfound = 0\n",
    "    # start with largest mag, and work our way down\n",
    "    for mdiff in np.arange(0.1,9,0.1):\n",
    "        msearch = LargeEqCutoff - mdiff\n",
    "        \n",
    "        # loop through eqs\n",
    "        for ev in EvtList:\n",
    "            # use events in the right bin\n",
    "            if ev[3] != ibin:\n",
    "                #print('not right bin, '+str(ev[3])+' is not '+str(ibin)) #debug\n",
    "                continue \n",
    "            # use events of right magnitude\n",
    "            if round(ev[4],1) != msearch:\n",
    "                continue\n",
    "            # now check to see if within some num of hours (2?) of another event already \n",
    "            trange = 2*60*60 # (utcdatetime uses 1 as a second... add 2 hours here)\n",
    "            for evTest in EvtListUse:\n",
    "                testTime = evTest[1]\n",
    "                if ev[1] < testTime+trange and ev[1] > testTime-trange:\n",
    "                    continue\n",
    "            # if it has passed all these tests, we should add it to the event list\n",
    "            print('Using event '+ev[0]+', M'+str(ev[4])+' for bin '+str(ibin))\n",
    "            EvtListUseFunc.append([ev[0],ev[1],ev[-1]]) #is it bad to append directly to this list? \n",
    "            # record that we've found an event... \n",
    "            Nfound += 1\n",
    "            # and check to see if we've found enough for this bin\n",
    "            if Nfound == Neq:\n",
    "                print('done with bin '+str(ibin))\n",
    "                return EvtListUseFunc\n",
    "    \n",
    "# the function is over\n",
    "\n",
    "# find number of eqs in bin with largest number of eq larger than the cutoff above\n",
    "maxeq = max(NumLargeEqs)\n",
    "maxbin = NumLargeEqs.index(maxeq)\n",
    "\n",
    "# now loop through all the other azi bins\n",
    "for ibin in range(0,NAziBin):\n",
    "    # get how many eqs we need for this bin\n",
    "    Neq = maxeq - NumLargeEqs[ibin]\n",
    "    # obviously if there are enough let's move on\n",
    "    if Neq == 0:\n",
    "        continue\n",
    "    # the function\n",
    "    EvtListUseNew = FindMoreEvents(ibin,Neq,EvtList,EvtListUse,LargeEqCutoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "#EventListOld = EvtListUse\n",
    "print(len(EvtList))\n",
    "#print(len(EventListOld))\n",
    "#EventListOld = []\n",
    "#a = EventListOld\n",
    "#print(len(a))\n",
    "#EvtListUseNew\n",
    "#print(EvtListUse)\n",
    "#print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[smi:service.iris.edu/fdsnws/event/1/query?eventid=3404092,\n",
       " smi:service.iris.edu/fdsnws/event/1/query?eventid=8853829,\n",
       " smi:service.iris.edu/fdsnws/event/1/query?eventid=8853879,\n",
       " smi:service.iris.edu/fdsnws/event/1/query?eventid=8854112]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EvtListUse\n",
    "a = [evt[-1] for evt in EvtListUse]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on event 201209262339\n",
      "Using comp BHZ for station G02B\n",
      "Using comp BH1 for station G02B\n",
      "Using comp BH2 for station G02B\n",
      "File /Users/whawley/Research/Blanco-SW/data/SAC_Events_G02B_Test/201209262339/201209262339.7D.G02B.BDH.sac exists; skipping\n",
      "Working on event 201209260645\n",
      "Using comp BHZ for station G02B\n"
     ]
    }
   ],
   "source": [
    "# now to download the data...\n",
    "\n",
    "# first get a list of all the eent ids\n",
    "IDs = [evt[-1] for evt in EvtListUse]\n",
    "# and a place to save the event list...\n",
    "f = open(EventsFileName,'w')\n",
    "\n",
    "# now the loop\n",
    "for iev in range(0,len(catIRIS)):\n",
    "    # skip the event if the id is not in the pared down list\n",
    "    if catIRIS[iev].resource_id not in IDs:\n",
    "        #print('not using event '+str(catIRIS[iev].resource_id)) #debug\n",
    "        continue\n",
    "    \n",
    "    # now same info gathering as before\n",
    "    if isCMT_params: \n",
    "        if isCentroid:\n",
    "            ior = 1\n",
    "        else:\n",
    "            ior = 0\n",
    "        \n",
    "        # if searching CMT catalogue, pull out necessary time info\n",
    "        time = catIRIS[iev].origins[0].time\n",
    "        date = datetime.strptime(str(time),'%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        month = calendar.month_abbr[date.month].lower()\n",
    "        year = str(date.year)\n",
    "        \n",
    "        # in case time is not exactly the same...\n",
    "        time1 = str(time-50) #+/- 50 seconds\n",
    "        time2 = str(time+50)\n",
    "        catCMT = obspy.read_events(\"https://www.ldeo.columbia.edu/~gcmt/projects/CMT/catalog/NEW_MONTHLY/\"+year+\"/\"+month+\"\"+year[2:4]+\".ndk\")\n",
    "        cat_filt = catCMT.filter('time > '+time1, 'time < '+time2, 'magnitude >= '+str(minMag-1))\n",
    "        if len(cat_filt)==0:\n",
    "            print('Cannot find event in GCMT catalogue... using IRIS')\n",
    "            cat = catIRIS[iev].copy()\n",
    "            ior = 0\n",
    "        else:\n",
    "            cat = cat_filt[0].copy()\n",
    "    else:\n",
    "        cat = catIRIS[iev].copy()\n",
    "        ior = 0\n",
    "    \n",
    "    # earthquake parameters\n",
    "    evT1 = cat.origins[ior].time\n",
    "    evT2 = evT1 + trLen\n",
    "    evdp = cat.origins[ior].depth\n",
    "    evla = cat.origins[ior].latitude\n",
    "    evlo = cat.origins[ior].longitude\n",
    "    mag = cat.magnitudes[0].mag\n",
    "    \n",
    "    # date for naming folders\n",
    "    date = datetime.strptime(str(evT1),'%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    evName = date.strftime('%Y%m%d%H%M')\n",
    "    evDir = EventsDataDir + evName + '/'\n",
    "    f.write(evName+'\\n')\n",
    "    if not os.path.exists(evDir):\n",
    "        os.makedirs(evDir)\n",
    "    print('Working on event '+evName)\n",
    "    #else: #...leave this out. Can download new station data for same event. \n",
    "        #print('Event '+evName+' exists, skipping...')\n",
    "        #continue\n",
    "        \n",
    "    # loop through stations\n",
    "    #for ista in range(0,len(inventoryX[0])):\n",
    "    for ista in range(0,len(inventory[0])):\n",
    "        stel = inventory[0].stations[ista].elevation\n",
    "        stla = inventory[0].stations[ista].latitude\n",
    "        stlo = inventory[0].stations[ista].longitude\n",
    "        station = inventory[0].stations[ista].code\n",
    "        vals = gps2dist_azimuth(lat1=stla, lon1=stlo, lat2=evla, lon2=evlo)\n",
    "        dist = vals[0]\n",
    "        baz = vals[1]\n",
    "        az = vals[2]\n",
    "        gcarc = locations2degrees(lat1=stla, long1=stlo, lat2=evla, long2=evlo)\n",
    "        \n",
    "        # Loop through components... \n",
    "        for comp in ChanList:\n",
    "\n",
    "            \n",
    "            # need to look for backups for some stations, particularly for 7D\n",
    "            if comp == 'BDH':\n",
    "                backupComp = 'BXH'\n",
    "                backupComp2 = 'BXH'\n",
    "                backupComp3 = 'BXH'\n",
    "            elif comp == 'LHZ':\n",
    "                backupComp = 'LXZ'\n",
    "                backupComp2 = 'BXZ'\n",
    "                backupComp3 = 'BHZ'\n",
    "            elif comp == 'LH1':\n",
    "                backupComp = 'LX1'\n",
    "                backupComp2 = 'BX1'\n",
    "                backupComp3 = 'BH1'\n",
    "            elif comp == 'LH2':\n",
    "                backupComp = 'LX2'\n",
    "                backupComp2 = 'BXH'\n",
    "                backupComp3 = 'BH2'\n",
    "                \n",
    "            sac_out = evDir + evName + '.' + network + '.' + station + '.' + comp + '.sac'\n",
    "            if os.path.exists(sac_out):\n",
    "                print(\"File \"+sac_out+\" exists; skipping\")\n",
    "                continue            \n",
    "            try:\n",
    "                st = client.get_waveforms(network=network, station=station, location='*', channel=comp, starttime=evT1, endtime=evT2, attach_response=True)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    st = client.get_waveforms(network=network, station=station, location='*', channel=backupComp, starttime=evT1, endtime=evT2, attach_response=True)\n",
    "                    print('Using comp '+backupComp+' for station '+station)\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        st = client.get_waveforms(network=network, station=station, location='*', channel=backupComp2, starttime=evT1, endtime=evT2, attach_response=True)\n",
    "                        print('Using comp '+backupComp2+' for station '+station)\n",
    "                    except Exception:\n",
    "                        try:\n",
    "                            st = client.get_waveforms(network=network, station=station, location='*', channel=backupComp3, starttime=evT1, endtime=evT2, attach_response=True)\n",
    "                            print('Using comp '+backupComp3+' for station '+station)\n",
    "                        except Exception:\n",
    "                            print('Missing data for station: ',station,'Comps:',comp,backupComp,backupComp2,backupComp3)\n",
    "                            continue\n",
    "                \n",
    "            # if BXH, change back ... doesn't matter; we downsample to 1 Hz anyway\n",
    "            if comp == 'BXH':\n",
    "                comp = 'BDH'\n",
    "                \n",
    "            # check for gaps\n",
    "            if len(st) > 1:\n",
    "                st.merge(method=1, fill_value=0)\n",
    "            sr = st[0].stats.sampling_rate\n",
    "            st.remove_response(output=\"DISP\", zero_mean=True, taper=True, taper_fraction=0.05, pre_filt=[LoFreq1,LoFreq2,sr/3,sr/2], water_level=60)\n",
    "            st.trim(starttime=evT1, endtime=evT2, pad=True, nearest_sample=False, fill_value=0)\n",
    "            st.detrend(type='demean')\n",
    "            st.detrend(type='linear')\n",
    "            st.taper(type='cosine',max_percentage=0.05)\n",
    "            \n",
    "            # downsample the trace\n",
    "            if isDownsamp==1:\n",
    "                st.filter('lowpass', freq=0.4*srNew, zerophase=True) #anti-alias\n",
    "                st.resample(sampling_rate=srNew)\n",
    "                st.detrend(type='demean')\n",
    "                st.detrend(type='linear')\n",
    "                st.taper(type='cosine',max_percentage=0.05)\n",
    "                \n",
    "            # convert to SAC\n",
    "            sac = SACTrace.from_obspy_trace(st[0])\n",
    "            sac.stel = stel\n",
    "            sac.stla = stla\n",
    "            sac.stlo = stlo\n",
    "            sac.evdp = evdp\n",
    "            sac.evla = evla\n",
    "            sac.evlo = evlo\n",
    "            sac.mag = mag\n",
    "            sac.dist = dist\n",
    "            sac.az = az\n",
    "            sac.baz = baz\n",
    "            sac.gcarc = gcarc\n",
    "            sac.kcmpnm = comp\n",
    "            \n",
    "\n",
    "            sac.write(sac_out)\n",
    "            \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
